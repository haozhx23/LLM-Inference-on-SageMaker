{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b666ee7",
   "metadata": {},
   "source": [
    "# Running stanford_alpaca on Amazon SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ae555b",
   "metadata": {},
   "source": [
    "This is a sample code to run stanford_alpaca on Amazon SageMaker, for demo or research use only!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5898f9b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Update sagemaker python sdk version\n",
    "!pip install -U sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6387eff3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "sagemaker_default_bucket = sess.default_bucket()\n",
    "\n",
    "account = sess.boto_session.client(\"sts\").get_caller_identity()[\"Account\"]\n",
    "region = sess.boto_session.region_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8846e1b3-e6e3-4a53-9e6a-743e01541671",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!rm -rf src\n",
    "!mkdir src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8de03012",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'stanford_alpaca'...\n",
      "remote: Enumerating objects: 129, done.\u001b[K\n",
      "remote: Counting objects: 100% (129/129), done.\u001b[K\n",
      "remote: Compressing objects: 100% (73/73), done.\u001b[K\n",
      "remote: Total 129 (delta 61), reused 104 (delta 55), pack-reused 0\u001b[K\n",
      "Receiving objects: 100% (129/129), 9.14 MiB | 5.84 MiB/s, done.\n",
      "Resolving deltas: 100% (61/61), done.\n"
     ]
    }
   ],
   "source": [
    "## download training script from github\n",
    "!cd src && git clone https://github.com/tatsu-lab/stanford_alpaca.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f35cec",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "## check data\n",
    "!head -n 44 src/stanford_alpaca/alpaca_data.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a3dca9",
   "metadata": {},
   "source": [
    "**Generate sample dataset for debug use.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "841d6c55",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/sample_dataset.json\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/sample_dataset.json\n",
    "[\n",
    "    {\n",
    "        \"instruction\": \"Give three tips for staying healthy.\",\n",
    "        \"input\": \"\",\n",
    "        \"output\": \"1.Eat a balanced diet and make sure to include plenty of fruits and vegetables. \\n2. Exercise regularly to keep your body active and strong. \\n3. Get enough sleep and maintain a consistent sleep schedule.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"What are the three primary colors?\",\n",
    "        \"input\": \"\",\n",
    "        \"output\": \"The three primary colors are red, blue, and yellow.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Describe the structure of an atom.\",\n",
    "        \"input\": \"\",\n",
    "        \"output\": \"An atom is made up of a nucleus, which contains protons and neutrons, surrounded by electrons that travel in orbits around the nucleus. The protons and neutrons have a positive charge, while the electrons have a negative charge, resulting in an overall neutral atom. The number of each particle determines the atomic number and the type of atom.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"How can we reduce air pollution?\",\n",
    "        \"input\": \"\",\n",
    "        \"output\": \"There are a number of ways to reduce air pollution, such as shifting to renewable energy sources, encouraging the use of public transportation, prohibiting the burning of fossil fuels, implementing policies to reduce emissions from industrial sources, and implementing vehicle emissions standards. Additionally, individuals can do their part to reduce air pollution by reducing car use, avoiding burning materials such as wood, and changing to energy efficient appliances.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Describe a time when you had to make a difficult decision.\",\n",
    "        \"input\": \"\",\n",
    "        \"output\": \"I had to make a difficult decision when I was working as a project manager at a construction company. I was in charge of a project that needed to be completed by a certain date in order to meet the client\\u2019s expectations. However, due to unexpected delays, we were not able to meet the deadline and so I had to make a difficult decision. I decided to extend the deadline, but I had to stretch the team\\u2019s resources even further and increase the budget. Although it was a risky decision, I ultimately decided to go ahead with it to ensure that the project was completed on time and that the client\\u2019s expectations were met. The project was eventually successfully completed and this was seen as a testament to my leadership and decision-making abilities.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Identify the odd one out.\",\n",
    "        \"input\": \"Twitter, Instagram, Telegram\",\n",
    "        \"output\": \"Telegram\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Explain why the following fraction is equivalent to 1/4\",\n",
    "        \"input\": \"4/16\",\n",
    "        \"output\": \"The fraction 4/16 is equivalent to 1/4 because both numerators and denominators are divisible by 4. Dividing both the top and bottom numbers by 4 yields the fraction 1/4.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Write a short story in third person narration about a protagonist who has to make an important career decision.\",\n",
    "        \"input\": \"\",\n",
    "        \"output\": \"John was at a crossroads in his life. He had just graduated college and was now facing the big decision of what career to pursue. After much deliberation, he decided that he wanted to be an accountant and help the financially disadvantaged. He had always been good with numbers and enjoyed seeing the tangible results of his work. \\n\\nJohn enrolled in accounting courses and initially found it quite challenging. He had to learn multiple systems and regulations quickly, but he worked hard and eventually excelled in his studies. After a few years, John started working at an accounting firm in his city. He was eager to put his knowledge of taxes and accounting to use in a real-world setting.\\n\\nJohn loved his job, as it let him express his creativity in finding strategies to save his clients money. After a few years at the firm, he became a senior accountant and was asked to manage bigger and more challenging cases. He was now a respected figure in the financial industry, but he still remembers when he was just a recent college graduate, unsure of the direction in which his life would take him.\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec88cf40",
   "metadata": {},
   "source": [
    "## Download pretrained model from HuggingFace Hub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3081c5b",
   "metadata": {},
   "source": [
    "To avoid download model from Huggingface hub failure, we download first and push those model files to S3 bucket first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a9df6a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0239c2d6",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "from pathlib import Path\n",
    "\n",
    "local_cache_path = Path(\"./model\")\n",
    "local_cache_path.mkdir(exist_ok=True)\n",
    "\n",
    "model_name = \"decapoda-research/llama-7b-hf\"#\n",
    "\n",
    "# Only download pytorch checkpoint files\n",
    "allow_patterns = [\"*.json\", \"*.pt\", \"*.bin\", \"*.model\"]\n",
    "\n",
    "model_download_path = snapshot_download(\n",
    "    repo_id=model_name,\n",
    "    cache_dir=local_cache_path,\n",
    "    allow_patterns=allow_patterns,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be704ef5",
   "metadata": {},
   "source": [
    "**Upload model files to S3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd09c171",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./model/models--decapoda-research--llama-7b-hf/snapshots/5f98eefcc80e437ef68d457ad7bf167c2c6a1348/config.json\n",
      "./model/models--decapoda-research--llama-7b-hf/snapshots/5f98eefcc80e437ef68d457ad7bf167c2c6a1348/\n"
     ]
    }
   ],
   "source": [
    "# Get the model files path\n",
    "import os\n",
    "from glob import glob\n",
    "\n",
    "local_model_path = None\n",
    "\n",
    "paths = os.walk(r'./model')\n",
    "for root, dirs, files in paths:\n",
    "    for file in files:\n",
    "        if file == 'config.json':\n",
    "            print(os.path.join(root,file))\n",
    "            local_model_path = str(os.path.join(root,file))[0:-11]\n",
    "            print(local_model_path)\n",
    "if local_model_path == None:\n",
    "    print(\"Model download may failed, please check prior step!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5716ad8",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cp model/models--decapoda-research--llama-7b-hf/snapshots/5f98eefcc80e437ef68d457ad7bf167c2c6a1348/config.json s3://sagemaker-us-east-1-633205212955/llama/pretrain/7B/config.json\n",
      "cp model/models--decapoda-research--llama-7b-hf/snapshots/5f98eefcc80e437ef68d457ad7bf167c2c6a1348/generation_config.json s3://sagemaker-us-east-1-633205212955/llama/pretrain/7B/generation_config.json\n",
      "cp model/models--decapoda-research--llama-7b-hf/snapshots/5f98eefcc80e437ef68d457ad7bf167c2c6a1348/tokenizer_config.json s3://sagemaker-us-east-1-633205212955/llama/pretrain/7B/tokenizer_config.json\n",
      "cp model/models--decapoda-research--llama-7b-hf/snapshots/5f98eefcc80e437ef68d457ad7bf167c2c6a1348/special_tokens_map.json s3://sagemaker-us-east-1-633205212955/llama/pretrain/7B/special_tokens_map.json\n",
      "cp model/models--decapoda-research--llama-7b-hf/snapshots/5f98eefcc80e437ef68d457ad7bf167c2c6a1348/pytorch_model.bin.index.json s3://sagemaker-us-east-1-633205212955/llama/pretrain/7B/pytorch_model.bin.index.json\n",
      "cp model/models--decapoda-research--llama-7b-hf/snapshots/5f98eefcc80e437ef68d457ad7bf167c2c6a1348/tokenizer.model s3://sagemaker-us-east-1-633205212955/llama/pretrain/7B/tokenizer.model\n",
      "cp model/models--decapoda-research--llama-7b-hf/snapshots/5f98eefcc80e437ef68d457ad7bf167c2c6a1348/pytorch_model-00023-of-00033.bin s3://sagemaker-us-east-1-633205212955/llama/pretrain/7B/pytorch_model-00023-of-00033.bin\n",
      "cp model/models--decapoda-research--llama-7b-hf/snapshots/5f98eefcc80e437ef68d457ad7bf167c2c6a1348/pytorch_model-00001-of-00033.bin s3://sagemaker-us-east-1-633205212955/llama/pretrain/7B/pytorch_model-00001-of-00033.bin\n",
      "cp model/models--decapoda-research--llama-7b-hf/snapshots/5f98eefcc80e437ef68d457ad7bf167c2c6a1348/pytorch_model-00024-of-00033.bin s3://sagemaker-us-east-1-633205212955/llama/pretrain/7B/pytorch_model-00024-of-00033.bin\n",
      "cp model/models--decapoda-research--llama-7b-hf/snapshots/5f98eefcc80e437ef68d457ad7bf167c2c6a1348/pytorch_model-00006-of-00033.bin s3://sagemaker-us-east-1-633205212955/llama/pretrain/7B/pytorch_model-00006-of-00033.bin\n",
      "cp model/models--decapoda-research--llama-7b-hf/snapshots/5f98eefcc80e437ef68d457ad7bf167c2c6a1348/pytorch_model-00020-of-00033.bin s3://sagemaker-us-east-1-633205212955/llama/pretrain/7B/pytorch_model-00020-of-00033.bin\n",
      "cp model/models--decapoda-research--llama-7b-hf/snapshots/5f98eefcc80e437ef68d457ad7bf167c2c6a1348/pytorch_model-00002-of-00033.bin s3://sagemaker-us-east-1-633205212955/llama/pretrain/7B/pytorch_model-00002-of-00033.bin\n",
      "cp model/models--decapoda-research--llama-7b-hf/snapshots/5f98eefcc80e437ef68d457ad7bf167c2c6a1348/pytorch_model-00016-of-00033.bin s3://sagemaker-us-east-1-633205212955/llama/pretrain/7B/pytorch_model-00016-of-00033.bin\n",
      "cp model/models--decapoda-research--llama-7b-hf/snapshots/5f98eefcc80e437ef68d457ad7bf167c2c6a1348/pytorch_model-00032-of-00033.bin s3://sagemaker-us-east-1-633205212955/llama/pretrain/7B/pytorch_model-00032-of-00033.bin\n",
      "cp model/models--decapoda-research--llama-7b-hf/snapshots/5f98eefcc80e437ef68d457ad7bf167c2c6a1348/pytorch_model-00028-of-00033.bin s3://sagemaker-us-east-1-633205212955/llama/pretrain/7B/pytorch_model-00028-of-00033.bin\n",
      "cp model/models--decapoda-research--llama-7b-hf/snapshots/5f98eefcc80e437ef68d457ad7bf167c2c6a1348/pytorch_model-00008-of-00033.bin s3://sagemaker-us-east-1-633205212955/llama/pretrain/7B/pytorch_model-00008-of-00033.bin\n",
      "cp model/models--decapoda-research--llama-7b-hf/snapshots/5f98eefcc80e437ef68d457ad7bf167c2c6a1348/pytorch_model-00012-of-00033.bin s3://sagemaker-us-east-1-633205212955/llama/pretrain/7B/pytorch_model-00012-of-00033.bin\n",
      "cp model/models--decapoda-research--llama-7b-hf/snapshots/5f98eefcc80e437ef68d457ad7bf167c2c6a1348/pytorch_model-00021-of-00033.bin s3://sagemaker-us-east-1-633205212955/llama/pretrain/7B/pytorch_model-00021-of-00033.bin\n",
      "cp model/models--decapoda-research--llama-7b-hf/snapshots/5f98eefcc80e437ef68d457ad7bf167c2c6a1348/pytorch_model-00010-of-00033.bin s3://sagemaker-us-east-1-633205212955/llama/pretrain/7B/pytorch_model-00010-of-00033.bin\n",
      "cp model/models--decapoda-research--llama-7b-hf/snapshots/5f98eefcc80e437ef68d457ad7bf167c2c6a1348/pytorch_model-00022-of-00033.bin s3://sagemaker-us-east-1-633205212955/llama/pretrain/7B/pytorch_model-00022-of-00033.bin\n",
      "cp model/models--decapoda-research--llama-7b-hf/snapshots/5f98eefcc80e437ef68d457ad7bf167c2c6a1348/pytorch_model-00011-of-00033.bin s3://sagemaker-us-east-1-633205212955/llama/pretrain/7B/pytorch_model-00011-of-00033.bin\n",
      "cp model/models--decapoda-research--llama-7b-hf/snapshots/5f98eefcc80e437ef68d457ad7bf167c2c6a1348/pytorch_model-00004-of-00033.bin s3://sagemaker-us-east-1-633205212955/llama/pretrain/7B/pytorch_model-00004-of-00033.bin\n",
      "cp model/models--decapoda-research--llama-7b-hf/snapshots/5f98eefcc80e437ef68d457ad7bf167c2c6a1348/pytorch_model-00019-of-00033.bin s3://sagemaker-us-east-1-633205212955/llama/pretrain/7B/pytorch_model-00019-of-00033.bin\n",
      "cp model/models--decapoda-research--llama-7b-hf/snapshots/5f98eefcc80e437ef68d457ad7bf167c2c6a1348/pytorch_model-00029-of-00033.bin s3://sagemaker-us-east-1-633205212955/llama/pretrain/7B/pytorch_model-00029-of-00033.bin\n",
      "cp model/models--decapoda-research--llama-7b-hf/snapshots/5f98eefcc80e437ef68d457ad7bf167c2c6a1348/pytorch_model-00003-of-00033.bin s3://sagemaker-us-east-1-633205212955/llama/pretrain/7B/pytorch_model-00003-of-00033.bin\n",
      "cp model/models--decapoda-research--llama-7b-hf/snapshots/5f98eefcc80e437ef68d457ad7bf167c2c6a1348/pytorch_model-00007-of-00033.bin s3://sagemaker-us-east-1-633205212955/llama/pretrain/7B/pytorch_model-00007-of-00033.bin\n",
      "cp model/models--decapoda-research--llama-7b-hf/snapshots/5f98eefcc80e437ef68d457ad7bf167c2c6a1348/pytorch_model-00015-of-00033.bin s3://sagemaker-us-east-1-633205212955/llama/pretrain/7B/pytorch_model-00015-of-00033.bin\n",
      "cp model/models--decapoda-research--llama-7b-hf/snapshots/5f98eefcc80e437ef68d457ad7bf167c2c6a1348/pytorch_model-00027-of-00033.bin s3://sagemaker-us-east-1-633205212955/llama/pretrain/7B/pytorch_model-00027-of-00033.bin\n",
      "cp model/models--decapoda-research--llama-7b-hf/snapshots/5f98eefcc80e437ef68d457ad7bf167c2c6a1348/pytorch_model-00026-of-00033.bin s3://sagemaker-us-east-1-633205212955/llama/pretrain/7B/pytorch_model-00026-of-00033.bin\n",
      "cp model/models--decapoda-research--llama-7b-hf/snapshots/5f98eefcc80e437ef68d457ad7bf167c2c6a1348/pytorch_model-00014-of-00033.bin s3://sagemaker-us-east-1-633205212955/llama/pretrain/7B/pytorch_model-00014-of-00033.bin\n",
      "cp model/models--decapoda-research--llama-7b-hf/snapshots/5f98eefcc80e437ef68d457ad7bf167c2c6a1348/pytorch_model-00033-of-00033.bin s3://sagemaker-us-east-1-633205212955/llama/pretrain/7B/pytorch_model-00033-of-00033.bin\n",
      "cp model/models--decapoda-research--llama-7b-hf/snapshots/5f98eefcc80e437ef68d457ad7bf167c2c6a1348/pytorch_model-00009-of-00033.bin s3://sagemaker-us-east-1-633205212955/llama/pretrain/7B/pytorch_model-00009-of-00033.bin\n",
      "cp model/models--decapoda-research--llama-7b-hf/snapshots/5f98eefcc80e437ef68d457ad7bf167c2c6a1348/pytorch_model-00005-of-00033.bin s3://sagemaker-us-east-1-633205212955/llama/pretrain/7B/pytorch_model-00005-of-00033.bin\n",
      "cp model/models--decapoda-research--llama-7b-hf/snapshots/5f98eefcc80e437ef68d457ad7bf167c2c6a1348/pytorch_model-00030-of-00033.bin s3://sagemaker-us-east-1-633205212955/llama/pretrain/7B/pytorch_model-00030-of-00033.bin\n",
      "cp model/models--decapoda-research--llama-7b-hf/snapshots/5f98eefcc80e437ef68d457ad7bf167c2c6a1348/pytorch_model-00025-of-00033.bin s3://sagemaker-us-east-1-633205212955/llama/pretrain/7B/pytorch_model-00025-of-00033.bin\n",
      "cp model/models--decapoda-research--llama-7b-hf/snapshots/5f98eefcc80e437ef68d457ad7bf167c2c6a1348/pytorch_model-00017-of-00033.bin s3://sagemaker-us-east-1-633205212955/llama/pretrain/7B/pytorch_model-00017-of-00033.bin\n",
      "cp model/models--decapoda-research--llama-7b-hf/snapshots/5f98eefcc80e437ef68d457ad7bf167c2c6a1348/pytorch_model-00031-of-00033.bin s3://sagemaker-us-east-1-633205212955/llama/pretrain/7B/pytorch_model-00031-of-00033.bin\n",
      "cp model/models--decapoda-research--llama-7b-hf/snapshots/5f98eefcc80e437ef68d457ad7bf167c2c6a1348/pytorch_model-00013-of-00033.bin s3://sagemaker-us-east-1-633205212955/llama/pretrain/7B/pytorch_model-00013-of-00033.bin\n",
      "cp model/models--decapoda-research--llama-7b-hf/snapshots/5f98eefcc80e437ef68d457ad7bf167c2c6a1348/pytorch_model-00018-of-00033.bin s3://sagemaker-us-east-1-633205212955/llama/pretrain/7B/pytorch_model-00018-of-00033.bin\n"
     ]
    }
   ],
   "source": [
    "%%script env sagemaker_default_bucket=$sagemaker_default_bucket local_model_path=$local_model_path bash\n",
    "\n",
    "chmod +x ./s5cmd\n",
    "./s5cmd sync ${local_model_path} s3://${sagemaker_default_bucket}/llama/pretrain/7B/\n",
    "\n",
    "rm -rf model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6163c2",
   "metadata": {},
   "source": [
    "### Modify Deepspeed config to save model properply.\n",
    "\n",
    "We will set ```stage3_gather_16bit_weights_on_model_save``` to ```Ture```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "65306cd3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "ds_config_file = './src/stanford_alpaca/configs/default_offload_opt_param.json'\n",
    "with open (ds_config_file, 'rb') as f:\n",
    "    ds_config = json.load(f)\n",
    "    f.close()\n",
    "    \n",
    "ds_config['zero_optimization']['stage3_gather_16bit_weights_on_model_save'] = True\n",
    "\n",
    "with open(ds_config_file, 'w') as f:\n",
    "    json.dump(ds_config, f, indent=2)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4edf0d",
   "metadata": {},
   "source": [
    "### Generate training entrypoint script\n",
    "\n",
    "**Note: DO NOT CHANGE BELOW VAlUE OF \"output_dir\" and \"cache_dir\", keep it \"/tmp/llama_out\" and \"/tmp\".**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2fc847d",
   "metadata": {},
   "source": [
    "Below is just a testing to fine-tune on a sample dataset (just 8 samples), you could change ```data_path``` to your dataset for furthur fine tune.\n",
    "\n",
    "For the dataset download, you could follow the way how to download pretrain model:\n",
    "```\n",
    "./s5cmd sync s3://$MODEL_S3_BUCKET/llama/pretrain/7B/* /tmp/llama_pretrain/\n",
    "```\n",
    "\n",
    "It is recommend to use the folder ```/tmp/dataset/```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "51a9887a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/train.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/train.sh\n",
    "#!/bin/bash\n",
    "\n",
    "pip install -r requirements.txt\n",
    "\n",
    "chmod +x ./s5cmd\n",
    "./s5cmd sync s3://$MODEL_S3_BUCKET/llama/pretrain/7B/* /tmp/llama_pretrain/\n",
    "\n",
    "torchrun --nproc_per_node=8 --master_port=12345 stanford_alpaca/train.py \\\n",
    "    --model_name_or_path \"/tmp/llama_pretrain/\" \\\n",
    "    --data_path sample_dataset.json \\ # absolute path is /opt/ml/code/src/sample_dataset.json\n",
    "    --bf16 True \\\n",
    "    --output_dir \"/tmp/llama_out\" \\\n",
    "    --num_train_epochs 3 \\\n",
    "    --per_device_train_batch_size 4 \\\n",
    "    --per_device_eval_batch_size 4 \\\n",
    "    --gradient_accumulation_steps 8 \\\n",
    "    --evaluation_strategy \"no\" \\\n",
    "    --save_strategy \"steps\" \\\n",
    "    --save_steps 2000 \\\n",
    "    --save_total_limit 1 \\\n",
    "    --learning_rate 2e-5 \\\n",
    "    --weight_decay 0. \\\n",
    "    --warmup_ratio 0.03 \\\n",
    "    --deepspeed \"./stanford_alpaca/configs/default_offload_opt_param.json\" \\\n",
    "    --tf32 True \\\n",
    "    --cache_dir '/tmp' \\\n",
    "    --report_to \"none\"\n",
    "\n",
    "if [ $? -eq 1 ]; then\n",
    "    echo \"Training script error, please check CloudWatch logs\"\n",
    "    exit 1\n",
    "fi\n",
    "\n",
    "./s5cmd sync /tmp/llama_out s3://$MODEL_S3_BUCKET/llama/output/$(date +%Y-%m-%d-%H-%M-%S)/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e40f25fc-80b8-4886-a50d-456e102d502c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!cp s5cmd src/\n",
    "!cp requirements.txt src/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "88b3b727-a48c-4a97-b70e-34cb25794e99",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# image_uri = '763104351884.dkr.ecr.us-east-1.amazonaws.com/pytorch-training:2.0.0-gpu-py310-cu118-ubuntu20.04-sagemaker'\n",
    "image_uri = \"763104351884.dkr.ecr.{}.amazonaws.com/huggingface-pytorch-training:1.13.1-transformers4.26.0-gpu-py39-cu117-ubuntu20.04\".format(region)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ddf5024",
   "metadata": {},
   "source": [
    "<!-- ### Modify train.py a little about how to save model\n",
    "\n",
    "Modify the model save methods in training script, change from \n",
    "\n",
    "```\n",
    "trainer.save_state()\n",
    "safe_save_model_for_hf_trainer(trainer=trainer, output_dir=training_args.output_dir)\n",
    "```\n",
    "\n",
    "to\n",
    "\n",
    "```\n",
    "tokenizer.save_pretrained(training_args.output_dir)\n",
    "trainer.save_model(training_args.output_dir)\n",
    "``` -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49434d18",
   "metadata": {},
   "source": [
    "**The modified training script**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954cbcf5",
   "metadata": {},
   "source": [
    "Everything is ready, let's launch the training job."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486913ec",
   "metadata": {},
   "source": [
    "## Create SageMaker Training Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b199e5",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:botocore.credentials:Found credentials from IAM Role: BaseNotebookInstanceEc2InstanceRole\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using provided s3_resource\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: stanford-alpaca-demo-2023-07-02-09-04-41-224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-02 09:04:46 Starting - Starting the training job..\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2023-07-02 09:04:56,140 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2023-07-02 09:04:56,202 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-07-02 09:04:56,211 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2023-07-02 09:04:56,213 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2023-07-02 09:04:56,858 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-07-02 09:04:56,931 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-07-02 09:04:57,002 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-07-02 09:04:57,012 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {},\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.p4d.24xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {},\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {},\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.p4d.24xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": true,\n",
      "    \"job_name\": \"stanford-alpaca-demo-2023-07-02-09-04-41-224\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-633205212955/stanford-alpaca-demo-2023-07-02-09-04-41-224/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train.sh\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 96,\n",
      "    \"num_gpus\": 8,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.p4d.24xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.p4d.24xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.sh\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train.sh\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p4d.24xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.p4d.24xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train.sh\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=96\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-633205212955/stanford-alpaca-demo-2023-07-02-09-04-41-224/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.p4d.24xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":true,\"job_name\":\"stanford-alpaca-demo-2023-07-02-09-04-41-224\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-633205212955/stanford-alpaca-demo-2023-07-02-09-04-41-224/source/sourcedir.tar.gz\",\"module_name\":\"train.sh\",\"network_interface_name\":\"eth0\",\"num_cpus\":96,\"num_gpus\":8,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p4d.24xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.sh\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python39.zip:/opt/conda/lib/python3.9:/opt/conda/lib/python3.9/lib-dynload:/opt/conda/lib/python3.9/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/bin/sh -c \"./train.sh \"\u001b[0m\n",
      "\u001b[34m[2023-07-02 09:04:58.535: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.26.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m2023-07-02 09:04:58,539 root         INFO     Using NamedTuple = typing._NamedTuple instead.\u001b[0m\n",
      "\u001b[34m2023-07-02 09:04:58,559 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[34mCollecting transformers@ git+https://github.com/huggingface/transformers@68d640f7c368bcaaaecfc678f11908ebbd3d6176\u001b[0m\n",
      "\u001b[34mCloning https://github.com/huggingface/transformers (to revision 68d640f7c368bcaaaecfc678f11908ebbd3d6176) to /tmp/pip-install-hxvmil9z/transformers_f8f90ae7321c428fa7bf14f57a87cdc2\u001b[0m\n",
      "\u001b[34mRunning command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers /tmp/pip-install-hxvmil9z/transformers_f8f90ae7321c428fa7bf14f57a87cdc2\u001b[0m\n",
      "\n",
      "2023-07-02 09:04:55 Downloading - Downloading input data\n",
      "2023-07-02 09:04:55 Training - Training image download completed. Training in progress.\u001b[34mRunning command git rev-parse -q --verify 'sha^68d640f7c368bcaaaecfc678f11908ebbd3d6176'\u001b[0m\n",
      "\u001b[34mRunning command git fetch -q https://github.com/huggingface/transformers 68d640f7c368bcaaaecfc678f11908ebbd3d6176\u001b[0m\n",
      "\u001b[34mRunning command git checkout -q 68d640f7c368bcaaaecfc678f11908ebbd3d6176\u001b[0m\n",
      "\u001b[34mResolved https://github.com/huggingface/transformers to commit 68d640f7c368bcaaaecfc678f11908ebbd3d6176\u001b[0m\n",
      "\u001b[34mInstalling build dependencies: started\u001b[0m\n",
      "\u001b[34mInstalling build dependencies: finished with status 'done'\u001b[0m\n",
      "\u001b[34mGetting requirements to build wheel: started\u001b[0m\n",
      "\u001b[34mGetting requirements to build wheel: finished with status 'done'\u001b[0m\n",
      "\u001b[34mPreparing metadata (pyproject.toml): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (pyproject.toml): finished with status 'done'\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy in /opt/conda/lib/python3.9/site-packages (from -r requirements.txt (line 1)) (1.23.5)\u001b[0m\n",
      "\u001b[34mCollecting rouge_score\u001b[0m\n",
      "\u001b[34mDownloading rouge_score-0.1.2.tar.gz (17 kB)\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCollecting fire\u001b[0m\n",
      "\u001b[34mDownloading fire-0.5.0.tar.gz (88 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 88.3/88.3 kB 6.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCollecting openai\u001b[0m\n",
      "\u001b[34mDownloading openai-0.27.8-py3-none-any.whl (73 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 73.6/73.6 kB 19.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch in /opt/conda/lib/python3.9/site-packages (from -r requirements.txt (line 6)) (1.13.1+cu117)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.9/site-packages (from -r requirements.txt (line 7)) (0.1.97)\u001b[0m\n",
      "\u001b[34mCollecting tokenizers>=0.13.3\u001b[0m\n",
      "\u001b[34mDownloading tokenizers-0.13.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.8/7.8 MB 96.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting wandb\u001b[0m\n",
      "\u001b[34mDownloading wandb-0.15.4-py3-none-any.whl (2.1 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.1/2.1 MB 100.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting deepspeed>=0.8.3\u001b[0m\n",
      "\u001b[34mDownloading deepspeed-0.9.5.tar.gz (809 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 809.9/809.9 kB 90.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCollecting absl-py\u001b[0m\n",
      "\u001b[34mDownloading absl_py-1.4.0-py3-none-any.whl (126 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 126.5/126.5 kB 31.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting nltk\u001b[0m\n",
      "\u001b[34mDownloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.5/1.5 MB 98.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.9/site-packages (from rouge_score->-r requirements.txt (line 2)) (1.16.0)\u001b[0m\n",
      "\u001b[34mCollecting termcolor\u001b[0m\n",
      "\u001b[34mDownloading termcolor-2.3.0-py3-none-any.whl (6.9 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm in /opt/conda/lib/python3.9/site-packages (from openai->-r requirements.txt (line 4)) (4.64.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiohttp in /opt/conda/lib/python3.9/site-packages (from openai->-r requirements.txt (line 4)) (3.8.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests>=2.20 in /opt/conda/lib/python3.9/site-packages (from openai->-r requirements.txt (line 4)) (2.28.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from transformers@ git+https://github.com/huggingface/transformers@68d640f7c368bcaaaecfc678f11908ebbd3d6176->-r requirements.txt (line 5)) (3.9.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /opt/conda/lib/python3.9/site-packages (from transformers@ git+https://github.com/huggingface/transformers@68d640f7c368bcaaaecfc678f11908ebbd3d6176->-r requirements.txt (line 5)) (0.12.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.9/site-packages (from transformers@ git+https://github.com/huggingface/transformers@68d640f7c368bcaaaecfc678f11908ebbd3d6176->-r requirements.txt (line 5)) (2022.10.31)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.9/site-packages (from transformers@ git+https://github.com/huggingface/transformers@68d640f7c368bcaaaecfc678f11908ebbd3d6176->-r requirements.txt (line 5)) (23.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.9/site-packages (from transformers@ git+https://github.com/huggingface/transformers@68d640f7c368bcaaaecfc678f11908ebbd3d6176->-r requirements.txt (line 5)) (5.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.9/site-packages (from torch->-r requirements.txt (line 6)) (4.4.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf!=4.21.0,<5,>=3.15.0 in /opt/conda/lib/python3.9/site-packages (from wandb->-r requirements.txt (line 9)) (3.20.2)\u001b[0m\n",
      "\u001b[34mCollecting pathtools\u001b[0m\n",
      "\u001b[34mDownloading pathtools-0.1.2.tar.gz (11 kB)\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCollecting sentry-sdk>=1.0.0\u001b[0m\n",
      "\u001b[34mDownloading sentry_sdk-1.26.0-py2.py3-none-any.whl (209 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 209.4/209.4 kB 45.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting setproctitle\u001b[0m\n",
      "\u001b[34mDownloading setproctitle-1.3.2-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: Click!=8.0.0,>=7.0 in /opt/conda/lib/python3.9/site-packages (from wandb->-r requirements.txt (line 9)) (8.1.2)\u001b[0m\n",
      "\u001b[34mCollecting docker-pycreds>=0.4.0\u001b[0m\n",
      "\u001b[34mDownloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: appdirs>=1.4.3 in /opt/conda/lib/python3.9/site-packages (from wandb->-r requirements.txt (line 9)) (1.4.4)\u001b[0m\n",
      "\u001b[34mCollecting GitPython!=3.1.29,>=1.0.0\u001b[0m\n",
      "\u001b[34mDownloading GitPython-3.1.31-py3-none-any.whl (184 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 184.3/184.3 kB 36.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: setuptools in /opt/conda/lib/python3.9/site-packages (from wandb->-r requirements.txt (line 9)) (65.6.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.9/site-packages (from wandb->-r requirements.txt (line 9)) (5.9.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: hjson in /opt/conda/lib/python3.9/site-packages (from deepspeed>=0.8.3->-r requirements.txt (line 10)) (3.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: ninja in /opt/conda/lib/python3.9/site-packages (from deepspeed>=0.8.3->-r requirements.txt (line 10)) (1.11.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: py-cpuinfo in /opt/conda/lib/python3.9/site-packages (from deepspeed>=0.8.3->-r requirements.txt (line 10)) (9.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pydantic<2.0.0 in /opt/conda/lib/python3.9/site-packages (from deepspeed>=0.8.3->-r requirements.txt (line 10)) (1.10.4)\u001b[0m\n",
      "\u001b[34mCollecting gitdb<5,>=4.0.1\u001b[0m\n",
      "\u001b[34mDownloading gitdb-4.0.10-py3-none-any.whl (62 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 62.7/62.7 kB 18.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests>=2.20->openai->-r requirements.txt (line 4)) (2022.12.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests>=2.20->openai->-r requirements.txt (line 4)) (1.26.14)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.9/site-packages (from requests>=2.20->openai->-r requirements.txt (line 4)) (2.1.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests>=2.20->openai->-r requirements.txt (line 4)) (3.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.9/site-packages (from aiohttp->openai->-r requirements.txt (line 4)) (1.3.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->openai->-r requirements.txt (line 4)) (1.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.9/site-packages (from aiohttp->openai->-r requirements.txt (line 4)) (1.3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.9/site-packages (from aiohttp->openai->-r requirements.txt (line 4)) (4.0.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->openai->-r requirements.txt (line 4)) (22.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.9/site-packages (from aiohttp->openai->-r requirements.txt (line 4)) (6.0.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: joblib in /opt/conda/lib/python3.9/site-packages (from nltk->rouge_score->-r requirements.txt (line 2)) (1.2.0)\u001b[0m\n",
      "\u001b[34mCollecting smmap<6,>=3.0.1\u001b[0m\n",
      "\u001b[34mDownloading smmap-5.0.0-py3-none-any.whl (24 kB)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: rouge_score, fire, transformers, deepspeed, pathtools\u001b[0m\n",
      "\u001b[34mBuilding wheel for rouge_score (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for rouge_score (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24935 sha256=897df195bc438f847f8e7471d10c4f3368547e57e75ab283af8d13f3096b8071\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/9b/3d/39/09558097d3119ca0a4d462df68f22c6f3c1b345ac63a09b86e\u001b[0m\n",
      "\u001b[34mBuilding wheel for fire (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for fire (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for fire: filename=fire-0.5.0-py2.py3-none-any.whl size=116931 sha256=cae0b1b387c7b1759dd565b812b7881c3388cf33addbc44b592b56e776af4815\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/f7/f1/89/b9ea2bf8f80ec027a88fef1d354b3816b4d3d29530988972f6\u001b[0m\n",
      "\u001b[34mBuilding wheel for transformers (pyproject.toml): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for transformers (pyproject.toml): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for transformers: filename=transformers-4.27.0.dev0-py3-none-any.whl size=6688302 sha256=4f0f0ac4922f822988bf84d8f2435203276c4baabf48ce8de81002b0d9c4842e\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/37/ca/0f/6253171f94bd5177f8b5c73f9cc1a9b7fef171c9948197d720\u001b[0m\n",
      "\u001b[34mBuilding wheel for deepspeed (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for deepspeed (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for deepspeed: filename=deepspeed-0.9.5-py3-none-any.whl size=844536 sha256=0a4f844ce87ea0bba7c3581801de1ea54896771fa7fc2e8e6826e0345b79135c\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/ec/e4/b5/37abf1bd14416a5785f7386bf78b96ae6ddc1fb24626bcc978\u001b[0m\n",
      "\u001b[34mBuilding wheel for pathtools (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for pathtools (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8791 sha256=4ff46dcb84c0e7a3ee2b91ef3ef544d770a54d0e62fbe287577b5a2412bdd28b\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/b7/0a/67/ada2a22079218c75a88361c0782855cc72aebc4d18d0289d05\u001b[0m\n",
      "\u001b[34mSuccessfully built rouge_score fire transformers deepspeed pathtools\u001b[0m\n",
      "\u001b[34mInstalling collected packages: tokenizers, pathtools, termcolor, smmap, setproctitle, sentry-sdk, nltk, docker-pycreds, absl-py, rouge_score, gitdb, fire, deepspeed, transformers, openai, GitPython, wandb\u001b[0m\n",
      "\u001b[34mAttempting uninstall: tokenizers\u001b[0m\n",
      "\u001b[34mFound existing installation: tokenizers 0.13.2\u001b[0m\n",
      "\u001b[34mUninstalling tokenizers-0.13.2:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled tokenizers-0.13.2\u001b[0m\n",
      "\u001b[34mAttempting uninstall: deepspeed\u001b[0m\n",
      "\u001b[34mFound existing installation: deepspeed 0.6.1+06f2048\u001b[0m\n",
      "\u001b[34mUninstalling deepspeed-0.6.1+06f2048:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled deepspeed-0.6.1+06f2048\u001b[0m\n",
      "\u001b[34mAttempting uninstall: transformers\u001b[0m\n",
      "\u001b[34mFound existing installation: transformers 4.26.0\u001b[0m\n",
      "\u001b[34mUninstalling transformers-4.26.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled transformers-4.26.0\u001b[0m\n",
      "\u001b[34mSuccessfully installed GitPython-3.1.31 absl-py-1.4.0 deepspeed-0.9.5 docker-pycreds-0.4.0 fire-0.5.0 gitdb-4.0.10 nltk-3.8.1 openai-0.27.8 pathtools-0.1.2 rouge_score-0.1.2 sentry-sdk-1.26.0 setproctitle-1.3.2 smmap-5.0.0 termcolor-2.3.0 tokenizers-0.13.3 transformers-4.27.0.dev0 wandb-0.15.4\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip is available: 23.0 -> 23.1.2\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-633205212955/llama/pretrain/7B/config.json /tmp/llama_pretrain/config.json\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-633205212955/llama/pretrain/7B/generation_config.json /tmp/llama_pretrain/generation_config.json\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-633205212955/llama/pretrain/7B/special_tokens_map.json /tmp/llama_pretrain/special_tokens_map.json\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-633205212955/llama/pretrain/7B/tokenizer_config.json /tmp/llama_pretrain/tokenizer_config.json\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-633205212955/llama/pretrain/7B/pytorch_model.bin.index.json /tmp/llama_pretrain/pytorch_model.bin.index.json\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-633205212955/llama/pretrain/7B/tokenizer.model /tmp/llama_pretrain/tokenizer.model\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-633205212955/llama/pretrain/7B/pytorch_model-00005-of-00033.bin /tmp/llama_pretrain/pytorch_model-00005-of-00033.bin\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-633205212955/llama/pretrain/7B/pytorch_model-00023-of-00033.bin /tmp/llama_pretrain/pytorch_model-00023-of-00033.bin\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-633205212955/llama/pretrain/7B/pytorch_model-00019-of-00033.bin /tmp/llama_pretrain/pytorch_model-00019-of-00033.bin\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-633205212955/llama/pretrain/7B/pytorch_model-00032-of-00033.bin /tmp/llama_pretrain/pytorch_model-00032-of-00033.bin\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-633205212955/llama/pretrain/7B/pytorch_model-00012-of-00033.bin /tmp/llama_pretrain/pytorch_model-00012-of-00033.bin\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-633205212955/llama/pretrain/7B/pytorch_model-00029-of-00033.bin /tmp/llama_pretrain/pytorch_model-00029-of-00033.bin\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-633205212955/llama/pretrain/7B/pytorch_model-00028-of-00033.bin /tmp/llama_pretrain/pytorch_model-00028-of-00033.bin\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-633205212955/llama/pretrain/7B/pytorch_model-00004-of-00033.bin /tmp/llama_pretrain/pytorch_model-00004-of-00033.bin\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-633205212955/llama/pretrain/7B/pytorch_model-00002-of-00033.bin /tmp/llama_pretrain/pytorch_model-00002-of-00033.bin\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-633205212955/llama/pretrain/7B/pytorch_model-00031-of-00033.bin /tmp/llama_pretrain/pytorch_model-00031-of-00033.bin\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-633205212955/llama/pretrain/7B/pytorch_model-00008-of-00033.bin /tmp/llama_pretrain/pytorch_model-00008-of-00033.bin\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-633205212955/llama/pretrain/7B/pytorch_model-00003-of-00033.bin /tmp/llama_pretrain/pytorch_model-00003-of-00033.bin\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-633205212955/llama/pretrain/7B/pytorch_model-00020-of-00033.bin /tmp/llama_pretrain/pytorch_model-00020-of-00033.bin\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-633205212955/llama/pretrain/7B/pytorch_model-00014-of-00033.bin /tmp/llama_pretrain/pytorch_model-00014-of-00033.bin\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-633205212955/llama/pretrain/7B/pytorch_model-00001-of-00033.bin /tmp/llama_pretrain/pytorch_model-00001-of-00033.bin\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-633205212955/llama/pretrain/7B/pytorch_model-00026-of-00033.bin /tmp/llama_pretrain/pytorch_model-00026-of-00033.bin\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-633205212955/llama/pretrain/7B/pytorch_model-00017-of-00033.bin /tmp/llama_pretrain/pytorch_model-00017-of-00033.bin\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-633205212955/llama/pretrain/7B/pytorch_model-00006-of-00033.bin /tmp/llama_pretrain/pytorch_model-00006-of-00033.bin\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-633205212955/llama/pretrain/7B/pytorch_model-00015-of-00033.bin /tmp/llama_pretrain/pytorch_model-00015-of-00033.bin\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-633205212955/llama/pretrain/7B/pytorch_model-00013-of-00033.bin /tmp/llama_pretrain/pytorch_model-00013-of-00033.bin\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-633205212955/llama/pretrain/7B/pytorch_model-00030-of-00033.bin /tmp/llama_pretrain/pytorch_model-00030-of-00033.bin\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-633205212955/llama/pretrain/7B/pytorch_model-00018-of-00033.bin /tmp/llama_pretrain/pytorch_model-00018-of-00033.bin\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-633205212955/llama/pretrain/7B/pytorch_model-00022-of-00033.bin /tmp/llama_pretrain/pytorch_model-00022-of-00033.bin\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-633205212955/llama/pretrain/7B/pytorch_model-00025-of-00033.bin /tmp/llama_pretrain/pytorch_model-00025-of-00033.bin\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-633205212955/llama/pretrain/7B/pytorch_model-00009-of-00033.bin /tmp/llama_pretrain/pytorch_model-00009-of-00033.bin\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-633205212955/llama/pretrain/7B/pytorch_model-00033-of-00033.bin /tmp/llama_pretrain/pytorch_model-00033-of-00033.bin\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-633205212955/llama/pretrain/7B/pytorch_model-00010-of-00033.bin /tmp/llama_pretrain/pytorch_model-00010-of-00033.bin\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-633205212955/llama/pretrain/7B/pytorch_model-00027-of-00033.bin /tmp/llama_pretrain/pytorch_model-00027-of-00033.bin\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-633205212955/llama/pretrain/7B/pytorch_model-00016-of-00033.bin /tmp/llama_pretrain/pytorch_model-00016-of-00033.bin\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-633205212955/llama/pretrain/7B/pytorch_model-00007-of-00033.bin /tmp/llama_pretrain/pytorch_model-00007-of-00033.bin\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-633205212955/llama/pretrain/7B/pytorch_model-00021-of-00033.bin /tmp/llama_pretrain/pytorch_model-00021-of-00033.bin\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-633205212955/llama/pretrain/7B/pytorch_model-00011-of-00033.bin /tmp/llama_pretrain/pytorch_model-00011-of-00033.bin\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-633205212955/llama/pretrain/7B/pytorch_model-00024-of-00033.bin /tmp/llama_pretrain/pytorch_model-00024-of-00033.bin\u001b[0m\n",
      "\u001b[34mWARNING:torch.distributed.run:\u001b[0m\n",
      "\u001b[34m*****************************************\u001b[0m\n",
      "\u001b[34mSetting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \u001b[0m\n",
      "\u001b[34m*****************************************\u001b[0m\n",
      "\u001b[34m[2023-07-02 09:05:42,352] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34m[2023-07-02 09:05:42,352] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34m[2023-07-02 09:05:42,352] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34m[2023-07-02 09:05:42,353] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34m[2023-07-02 09:05:42,354] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34m[2023-07-02 09:05:42,355] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34m[2023-07-02 09:05:42,358] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34m[2023-07-02 09:05:42,358] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34m[2023-07-02 09:05:46,280] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\u001b[0m\n",
      "\u001b[34m[2023-07-02 09:05:46,280] [INFO] [comm.py:594:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[34m[2023-07-02 09:05:46,292] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\u001b[0m\n",
      "\u001b[34m[2023-07-02 09:05:46,292] [INFO] [comm.py:594:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[34m[2023-07-02 09:05:46,317] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\u001b[0m\n",
      "\u001b[34m[2023-07-02 09:05:46,317] [INFO] [comm.py:594:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[34m[2023-07-02 09:05:46,317] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\u001b[0m\n",
      "\u001b[34m[2023-07-02 09:05:46,317] [INFO] [comm.py:594:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[34m[2023-07-02 09:05:46,317] [INFO] [comm.py:625:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\u001b[0m\n",
      "\u001b[34m[2023-07-02 09:05:46,317] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\u001b[0m\n",
      "\u001b[34m[2023-07-02 09:05:46,317] [INFO] [comm.py:594:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[34m[2023-07-02 09:05:46,318] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\u001b[0m\n",
      "\u001b[34m[2023-07-02 09:05:46,318] [INFO] [comm.py:594:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[34m[2023-07-02 09:05:46,324] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\u001b[0m\n",
      "\u001b[34m[2023-07-02 09:05:46,324] [INFO] [comm.py:594:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[34m[2023-07-02 09:05:46,324] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\u001b[0m\n",
      "\u001b[34m[2023-07-02 09:05:46,324] [INFO] [comm.py:594:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[34mNCCL version 2.14.3+cuda11.7\u001b[0m\n",
      "\u001b[34m[2023-07-02 09:05:51,881] [INFO] [partition_parameters.py:453:__exit__] finished initializing model with 6.74B parameters\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   3%|▎         | 1/33 [00:00<00:25,  1.25it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   3%|▎         | 1/33 [00:00<00:25,  1.28it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   3%|▎         | 1/33 [00:00<00:25,  1.23it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   3%|▎         | 1/33 [00:00<00:25,  1.24it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   3%|▎         | 1/33 [00:00<00:26,  1.22it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   3%|▎         | 1/33 [00:00<00:26,  1.20it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   3%|▎         | 1/33 [00:00<00:26,  1.20it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   3%|▎         | 1/33 [00:00<00:26,  1.19it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   6%|▌         | 2/33 [00:01<00:24,  1.25it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   6%|▌         | 2/33 [00:01<00:24,  1.26it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   6%|▌         | 2/33 [00:01<00:25,  1.23it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   6%|▌         | 2/33 [00:01<00:25,  1.23it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   6%|▌         | 2/33 [00:01<00:25,  1.23it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   6%|▌         | 2/33 [00:01<00:25,  1.21it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   6%|▌         | 2/33 [00:01<00:25,  1.21it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   6%|▌         | 2/33 [00:01<00:25,  1.21it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   9%|▉         | 3/33 [00:02<00:24,  1.24it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   9%|▉         | 3/33 [00:02<00:24,  1.24it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   9%|▉         | 3/33 [00:02<00:24,  1.23it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   9%|▉         | 3/33 [00:02<00:24,  1.23it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   9%|▉         | 3/33 [00:02<00:24,  1.23it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   9%|▉         | 3/33 [00:02<00:24,  1.22it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   9%|▉         | 3/33 [00:02<00:24,  1.22it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   9%|▉         | 3/33 [00:02<00:24,  1.22it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  12%|█▏        | 4/33 [00:03<00:23,  1.23it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  12%|█▏        | 4/33 [00:03<00:23,  1.24it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  12%|█▏        | 4/33 [00:03<00:23,  1.23it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  12%|█▏        | 4/33 [00:03<00:23,  1.23it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  12%|█▏        | 4/33 [00:03<00:23,  1.22it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  12%|█▏        | 4/33 [00:03<00:23,  1.22it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  12%|█▏        | 4/33 [00:03<00:23,  1.22it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  12%|█▏        | 4/33 [00:03<00:23,  1.22it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  15%|█▌        | 5/33 [00:04<00:22,  1.23it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  15%|█▌        | 5/33 [00:04<00:22,  1.23it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  15%|█▌        | 5/33 [00:04<00:22,  1.23it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  15%|█▌        | 5/33 [00:04<00:22,  1.23it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  15%|█▌        | 5/33 [00:04<00:22,  1.23it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  15%|█▌        | 5/33 [00:04<00:22,  1.22it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  15%|█▌        | 5/33 [00:04<00:22,  1.22it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  15%|█▌        | 5/33 [00:04<00:22,  1.22it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  18%|█▊        | 6/33 [00:04<00:22,  1.22it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  18%|█▊        | 6/33 [00:04<00:22,  1.22it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  18%|█▊        | 6/33 [00:04<00:22,  1.21it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  18%|█▊        | 6/33 [00:04<00:22,  1.21it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  18%|█▊        | 6/33 [00:04<00:22,  1.21it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  18%|█▊        | 6/33 [00:04<00:22,  1.21it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  18%|█▊        | 6/33 [00:04<00:22,  1.21it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  18%|█▊        | 6/33 [00:04<00:22,  1.21it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  21%|██        | 7/33 [00:05<00:21,  1.21it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  21%|██        | 7/33 [00:05<00:21,  1.21it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  21%|██        | 7/33 [00:05<00:21,  1.21it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  21%|██        | 7/33 [00:05<00:21,  1.21it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  21%|██        | 7/33 [00:05<00:21,  1.21it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  21%|██        | 7/33 [00:05<00:21,  1.20it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  21%|██        | 7/33 [00:05<00:21,  1.20it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  21%|██        | 7/33 [00:05<00:21,  1.20it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  24%|██▍       | 8/33 [00:06<00:20,  1.21it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  24%|██▍       | 8/33 [00:06<00:20,  1.21it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  24%|██▍       | 8/33 [00:06<00:20,  1.21it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  24%|██▍       | 8/33 [00:06<00:20,  1.21it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  24%|██▍       | 8/33 [00:06<00:20,  1.21it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  24%|██▍       | 8/33 [00:06<00:20,  1.21it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  24%|██▍       | 8/33 [00:06<00:20,  1.21it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  24%|██▍       | 8/33 [00:06<00:20,  1.21it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  27%|██▋       | 9/33 [00:07<00:19,  1.21it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  27%|██▋       | 9/33 [00:07<00:19,  1.21it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  27%|██▋       | 9/33 [00:07<00:19,  1.21it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  27%|██▋       | 9/33 [00:07<00:19,  1.21it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  27%|██▋       | 9/33 [00:07<00:19,  1.21it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  27%|██▋       | 9/33 [00:07<00:19,  1.21it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  27%|██▋       | 9/33 [00:07<00:19,  1.21it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  27%|██▋       | 9/33 [00:07<00:19,  1.21it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  30%|███       | 10/33 [00:08<00:18,  1.22it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  30%|███       | 10/33 [00:08<00:18,  1.22it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  30%|███       | 10/33 [00:08<00:18,  1.22it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  30%|███       | 10/33 [00:08<00:18,  1.22it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  30%|███       | 10/33 [00:08<00:18,  1.22it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  30%|███       | 10/33 [00:08<00:18,  1.22it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  30%|███       | 10/33 [00:08<00:18,  1.22it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  30%|███       | 10/33 [00:08<00:18,  1.22it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  33%|███▎      | 11/33 [00:09<00:18,  1.22it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  33%|███▎      | 11/33 [00:08<00:18,  1.22it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  33%|███▎      | 11/33 [00:09<00:18,  1.22it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  33%|███▎      | 11/33 [00:09<00:18,  1.22it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  33%|███▎      | 11/33 [00:09<00:18,  1.22it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  33%|███▎      | 11/33 [00:09<00:18,  1.22it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  33%|███▎      | 11/33 [00:09<00:18,  1.22it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  33%|███▎      | 11/33 [00:09<00:18,  1.22it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  36%|███▋      | 12/33 [00:09<00:17,  1.22it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  36%|███▋      | 12/33 [00:09<00:17,  1.22it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  36%|███▋      | 12/33 [00:09<00:17,  1.22it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  36%|███▋      | 12/33 [00:09<00:17,  1.22it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  36%|███▋      | 12/33 [00:09<00:17,  1.22it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  36%|███▋      | 12/33 [00:09<00:17,  1.22it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  36%|███▋      | 12/33 [00:09<00:17,  1.22it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  36%|███▋      | 12/33 [00:09<00:17,  1.22it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  39%|███▉      | 13/33 [00:10<00:16,  1.22it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  39%|███▉      | 13/33 [00:10<00:16,  1.22it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  39%|███▉      | 13/33 [00:10<00:16,  1.22it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  39%|███▉      | 13/33 [00:10<00:16,  1.22it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  39%|███▉      | 13/33 [00:10<00:16,  1.22it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  39%|███▉      | 13/33 [00:10<00:16,  1.22it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  39%|███▉      | 13/33 [00:10<00:16,  1.22it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  39%|███▉      | 13/33 [00:10<00:16,  1.22it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  42%|████▏     | 14/33 [00:11<00:15,  1.22it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  42%|████▏     | 14/33 [00:11<00:15,  1.22it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  42%|████▏     | 14/33 [00:11<00:15,  1.22it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  42%|████▏     | 14/33 [00:11<00:15,  1.22it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  42%|████▏     | 14/33 [00:11<00:15,  1.22it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  42%|████▏     | 14/33 [00:11<00:15,  1.22it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  42%|████▏     | 14/33 [00:11<00:15,  1.22it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  42%|████▏     | 14/33 [00:11<00:15,  1.22it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  45%|████▌     | 15/33 [00:12<00:14,  1.22it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  45%|████▌     | 15/33 [00:12<00:14,  1.22it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  45%|████▌     | 15/33 [00:12<00:14,  1.22it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  45%|████▌     | 15/33 [00:12<00:14,  1.22it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  45%|████▌     | 15/33 [00:12<00:14,  1.22it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  45%|████▌     | 15/33 [00:12<00:14,  1.23it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  45%|████▌     | 15/33 [00:12<00:14,  1.22it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  45%|████▌     | 15/33 [00:12<00:14,  1.22it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  48%|████▊     | 16/33 [00:13<00:13,  1.22it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  48%|████▊     | 16/33 [00:13<00:13,  1.22it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  48%|████▊     | 16/33 [00:13<00:13,  1.22it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  48%|████▊     | 16/33 [00:13<00:13,  1.22it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  48%|████▊     | 16/33 [00:13<00:13,  1.22it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  48%|████▊     | 16/33 [00:13<00:13,  1.22it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  48%|████▊     | 16/33 [00:13<00:13,  1.22it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  48%|████▊     | 16/33 [00:13<00:13,  1.22it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  52%|█████▏    | 17/33 [00:13<00:13,  1.22it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  52%|█████▏    | 17/33 [00:13<00:13,  1.22it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  52%|█████▏    | 17/33 [00:13<00:13,  1.22it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  52%|█████▏    | 17/33 [00:13<00:13,  1.22it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  52%|█████▏    | 17/33 [00:13<00:13,  1.22it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  52%|█████▏    | 17/33 [00:13<00:13,  1.22it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  52%|█████▏    | 17/33 [00:13<00:13,  1.22it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  52%|█████▏    | 17/33 [00:13<00:13,  1.22it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  55%|█████▍    | 18/33 [00:14<00:12,  1.22it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  55%|█████▍    | 18/33 [00:14<00:12,  1.22it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  55%|█████▍    | 18/33 [00:14<00:12,  1.22it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  55%|█████▍    | 18/33 [00:14<00:12,  1.22it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  55%|█████▍    | 18/33 [00:14<00:12,  1.23it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  55%|█████▍    | 18/33 [00:14<00:12,  1.23it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  55%|█████▍    | 18/33 [00:14<00:12,  1.22it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  55%|█████▍    | 18/33 [00:14<00:12,  1.22it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  58%|█████▊    | 19/33 [00:15<00:11,  1.23it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  58%|█████▊    | 19/33 [00:15<00:11,  1.23it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  58%|█████▊    | 19/33 [00:15<00:11,  1.22it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  58%|█████▊    | 19/33 [00:15<00:11,  1.22it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  58%|█████▊    | 19/33 [00:15<00:11,  1.22it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  58%|█████▊    | 19/33 [00:15<00:11,  1.22it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  58%|█████▊    | 19/33 [00:15<00:11,  1.23it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  58%|█████▊    | 19/33 [00:15<00:11,  1.23it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  61%|██████    | 20/33 [00:16<00:10,  1.23it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  61%|██████    | 20/33 [00:16<00:10,  1.23it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  61%|██████    | 20/33 [00:16<00:10,  1.23it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  61%|██████    | 20/33 [00:16<00:10,  1.23it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  61%|██████    | 20/33 [00:16<00:10,  1.23it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  61%|██████    | 20/33 [00:16<00:10,  1.23it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  61%|██████    | 20/33 [00:16<00:10,  1.22it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  61%|██████    | 20/33 [00:16<00:10,  1.23it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  64%|██████▎   | 21/33 [00:17<00:09,  1.23it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  64%|██████▎   | 21/33 [00:17<00:09,  1.23it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  64%|██████▎   | 21/33 [00:17<00:09,  1.23it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  64%|██████▎   | 21/33 [00:17<00:09,  1.23it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  64%|██████▎   | 21/33 [00:17<00:09,  1.23it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  64%|██████▎   | 21/33 [00:17<00:09,  1.23it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  64%|██████▎   | 21/33 [00:17<00:09,  1.23it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  64%|██████▎   | 21/33 [00:17<00:09,  1.23it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  67%|██████▋   | 22/33 [00:17<00:08,  1.23it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  67%|██████▋   | 22/33 [00:17<00:08,  1.23it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  67%|██████▋   | 22/33 [00:18<00:08,  1.23it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  67%|██████▋   | 22/33 [00:17<00:08,  1.23it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  67%|██████▋   | 22/33 [00:18<00:08,  1.23it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  67%|██████▋   | 22/33 [00:18<00:08,  1.22it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  67%|██████▋   | 22/33 [00:18<00:08,  1.23it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  67%|██████▋   | 22/33 [00:18<00:08,  1.23it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  70%|██████▉   | 23/33 [00:18<00:08,  1.23it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  70%|██████▉   | 23/33 [00:18<00:08,  1.23it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  70%|██████▉   | 23/33 [00:18<00:08,  1.23it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  70%|██████▉   | 23/33 [00:18<00:08,  1.23it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  70%|██████▉   | 23/33 [00:18<00:08,  1.23it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  70%|██████▉   | 23/33 [00:18<00:08,  1.23it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  70%|██████▉   | 23/33 [00:18<00:08,  1.23it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  70%|██████▉   | 23/33 [00:18<00:08,  1.23it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  73%|███████▎  | 24/33 [00:19<00:07,  1.23it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  73%|███████▎  | 24/33 [00:19<00:07,  1.23it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  73%|███████▎  | 24/33 [00:19<00:07,  1.23it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  73%|███████▎  | 24/33 [00:19<00:07,  1.23it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  73%|███████▎  | 24/33 [00:19<00:07,  1.23it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  73%|███████▎  | 24/33 [00:19<00:07,  1.23it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  73%|███████▎  | 24/33 [00:19<00:07,  1.23it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  73%|███████▎  | 24/33 [00:19<00:07,  1.23it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  76%|███████▌  | 25/33 [00:20<00:06,  1.23it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  76%|███████▌  | 25/33 [00:20<00:06,  1.23it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  76%|███████▌  | 25/33 [00:20<00:06,  1.23it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  76%|███████▌  | 25/33 [00:20<00:06,  1.23it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  76%|███████▌  | 25/33 [00:20<00:06,  1.23it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  76%|███████▌  | 25/33 [00:20<00:06,  1.23it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  76%|███████▌  | 25/33 [00:20<00:06,  1.23it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  76%|███████▌  | 25/33 [00:20<00:06,  1.23it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  79%|███████▉  | 26/33 [00:21<00:05,  1.23it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  79%|███████▉  | 26/33 [00:21<00:05,  1.23it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  79%|███████▉  | 26/33 [00:21<00:05,  1.23it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  79%|███████▉  | 26/33 [00:21<00:05,  1.23it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  79%|███████▉  | 26/33 [00:21<00:05,  1.23it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  79%|███████▉  | 26/33 [00:21<00:05,  1.23it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  79%|███████▉  | 26/33 [00:21<00:05,  1.23it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  79%|███████▉  | 26/33 [00:21<00:05,  1.23it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  82%|████████▏ | 27/33 [00:22<00:04,  1.23it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  82%|████████▏ | 27/33 [00:22<00:04,  1.23it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  82%|████████▏ | 27/33 [00:22<00:04,  1.23it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  82%|████████▏ | 27/33 [00:22<00:04,  1.23it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  82%|████████▏ | 27/33 [00:22<00:04,  1.23it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  82%|████████▏ | 27/33 [00:22<00:04,  1.23it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  82%|████████▏ | 27/33 [00:22<00:04,  1.23it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  82%|████████▏ | 27/33 [00:22<00:04,  1.23it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  85%|████████▍ | 28/33 [00:22<00:04,  1.22it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  85%|████████▍ | 28/33 [00:22<00:04,  1.22it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  85%|████████▍ | 28/33 [00:22<00:04,  1.22it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  85%|████████▍ | 28/33 [00:22<00:04,  1.22it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  85%|████████▍ | 28/33 [00:22<00:04,  1.22it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  85%|████████▍ | 28/33 [00:22<00:04,  1.23it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  85%|████████▍ | 28/33 [00:22<00:04,  1.22it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  85%|████████▍ | 28/33 [00:22<00:04,  1.23it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  88%|████████▊ | 29/33 [00:23<00:03,  1.23it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  88%|████████▊ | 29/33 [00:23<00:03,  1.23it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  88%|████████▊ | 29/33 [00:23<00:03,  1.23it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  88%|████████▊ | 29/33 [00:23<00:03,  1.23it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  88%|████████▊ | 29/33 [00:23<00:03,  1.23it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  88%|████████▊ | 29/33 [00:23<00:03,  1.22it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  88%|████████▊ | 29/33 [00:23<00:03,  1.23it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  88%|████████▊ | 29/33 [00:23<00:03,  1.23it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  91%|█████████ | 30/33 [00:24<00:02,  1.23it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  91%|█████████ | 30/33 [00:24<00:02,  1.23it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  91%|█████████ | 30/33 [00:24<00:02,  1.23it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  91%|█████████ | 30/33 [00:24<00:02,  1.23it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  91%|█████████ | 30/33 [00:24<00:02,  1.23it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  91%|█████████ | 30/33 [00:24<00:02,  1.23it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  91%|█████████ | 30/33 [00:24<00:02,  1.23it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  91%|█████████ | 30/33 [00:24<00:02,  1.23it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  94%|█████████▍| 31/33 [00:25<00:01,  1.23it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  94%|█████████▍| 31/33 [00:25<00:01,  1.23it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  94%|█████████▍| 31/33 [00:25<00:01,  1.23it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  94%|█████████▍| 31/33 [00:25<00:01,  1.23it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  94%|█████████▍| 31/33 [00:25<00:01,  1.23it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  94%|█████████▍| 31/33 [00:25<00:01,  1.23it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  94%|█████████▍| 31/33 [00:25<00:01,  1.23it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  94%|█████████▍| 31/33 [00:25<00:01,  1.23it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  97%|█████████▋| 32/33 [00:26<00:00,  1.22it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  97%|█████████▋| 32/33 [00:26<00:00,  1.22it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  97%|█████████▋| 32/33 [00:26<00:00,  1.22it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  97%|█████████▋| 32/33 [00:26<00:00,  1.22it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  97%|█████████▋| 32/33 [00:26<00:00,  1.22it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  97%|█████████▋| 32/33 [00:26<00:00,  1.22it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  97%|█████████▋| 32/33 [00:26<00:00,  1.22it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  97%|█████████▋| 32/33 [00:26<00:00,  1.22it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 33/33 [00:27<00:00,  1.14it/s]#015Loading checkpoint shards: 100%|██████████| 33/33 [00:27<00:00,  1.22it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 33/33 [00:27<00:00,  1.14it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 33/33 [00:27<00:00,  1.22it/s]\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 33/33 [00:27<00:00,  1.14it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 33/33 [00:27<00:00,  1.21it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 33/33 [00:27<00:00,  1.14it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 33/33 [00:27<00:00,  1.21it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 33/33 [00:27<00:00,  1.14it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 33/33 [00:27<00:00,  1.21it/s]\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 33/33 [00:27<00:00,  1.14it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 33/33 [00:27<00:00,  1.21it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 33/33 [00:27<00:00,  1.14it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 33/33 [00:27<00:00,  1.21it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 33/33 [00:27<00:00,  1.14it/s]#015Loading checkpoint shards: 100%|██████████| 33/33 [00:27<00:00,  1.21it/s]\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mWARNING:root:Loading data...\u001b[0m\n",
      "\u001b[34mWARNING:root:Loading data...\u001b[0m\n",
      "\u001b[34mWARNING:root:Loading data...\u001b[0m\n",
      "\u001b[34mWARNING:root:Loading data...\u001b[0m\n",
      "\u001b[34mWARNING:root:Loading data...\u001b[0m\n",
      "\u001b[34mWARNING:root:Loading data...\u001b[0m\n",
      "\u001b[34mWARNING:root:Loading data...\u001b[0m\n",
      "\u001b[34mWARNING:root:Loading data...\u001b[0m\n",
      "\u001b[34mWARNING:root:Formatting inputs...\u001b[0m\n",
      "\u001b[34mWARNING:root:Formatting inputs...\u001b[0m\n",
      "\u001b[34mWARNING:root:Formatting inputs...\u001b[0m\n",
      "\u001b[34mWARNING:root:Formatting inputs...\u001b[0m\n",
      "\u001b[34mWARNING:root:Formatting inputs...\u001b[0m\n",
      "\u001b[34mWARNING:root:Formatting inputs...\u001b[0m\n",
      "\u001b[34mWARNING:root:Formatting inputs...\u001b[0m\n",
      "\u001b[34mWARNING:root:Formatting inputs...\u001b[0m\n",
      "\u001b[34mWARNING:root:Tokenizing inputs... This may take some time...\u001b[0m\n",
      "\u001b[34mWARNING:root:Tokenizing inputs... This may take some time...\u001b[0m\n",
      "\u001b[34mWARNING:root:Tokenizing inputs... This may take some time...\u001b[0m\n",
      "\u001b[34mWARNING:root:Tokenizing inputs... This may take some time...\u001b[0m\n",
      "\u001b[34mWARNING:root:Tokenizing inputs... This may take some time...\u001b[0m\n",
      "\u001b[34mWARNING:root:Tokenizing inputs... This may take some time...\u001b[0m\n",
      "\u001b[34mWARNING:root:Tokenizing inputs... This may take some time...\u001b[0m\n",
      "\u001b[34mWARNING:root:Tokenizing inputs... This may take some time...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mCreating extension directory /root/.cache/torch_extensions/py39_cu117/cpu_adam...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mDetected CUDA files, patching ldflags\u001b[0m\n",
      "\u001b[34mEmitting ninja build file /root/.cache/torch_extensions/py39_cu117/cpu_adam/build.ninja...\u001b[0m\n",
      "\u001b[34mBuilding extension module cpu_adam...\u001b[0m\n",
      "\u001b[34mAllowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\u001b[0m\n",
      "\u001b[34m[1/3] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=cpu_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/includes -I/usr/local/cuda/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.9/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.9 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_80,code=compute_80 -gencode=arch=compute_80,code=sm_80 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_80,code=compute_80 -DBF16_AVAILABLE -c /opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/common/custom_cuda_kernel.cu -o custom_cuda_kernel.cuda.o\u001b[0m\n",
      "\u001b[34m[2/3] c++ -MMD -MF cpu_adam.o.d -DTORCH_EXTENSION_NAME=cpu_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/includes -I/usr/local/cuda/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.9/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.9 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++14 -O3 -std=c++17 -g -Wno-reorder -L/usr/local/cuda/lib64 -lcudart -lcublas -g -march=native -fopenmp -D__AVX512__ -D__ENABLE_CUDA__ -DBF16_AVAILABLE -c /opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/adam/cpu_adam.cpp -o cpu_adam.o\u001b[0m\n",
      "\u001b[34m[3/3] c++ cpu_adam.o custom_cuda_kernel.cuda.o -shared -lcurand -L/opt/conda/lib/python3.9/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda_cu -ltorch_cuda_cpp -ltorch -ltorch_python -L/usr/local/cuda/lib64 -lcudart -o cpu_adam.so\u001b[0m\n",
      "\u001b[34mLoading extension module cpu_adam...\u001b[0m\n",
      "\u001b[34mTime to load cpu_adam op: 30.414686679840088 seconds\u001b[0m\n",
      "\u001b[34mLoading extension module cpu_adam...\u001b[0m\n",
      "\u001b[34mTime to load cpu_adam op: 30.287842988967896 seconds\u001b[0m\n",
      "\u001b[34mLoading extension module cpu_adam...\u001b[0m\n",
      "\u001b[34mTime to load cpu_adam op: 30.385645151138306 seconds\u001b[0m\n",
      "\u001b[34mLoading extension module cpu_adam...\u001b[0m\n",
      "\u001b[34mTime to load cpu_adam op: 30.39238667488098 seconds\u001b[0m\n",
      "\u001b[34mLoading extension module cpu_adam...\u001b[0m\n",
      "\u001b[34mLoading extension module cpu_adam...\u001b[0m\n",
      "\u001b[34mTime to load cpu_adam op: 30.39937472343445 seconds\u001b[0m\n",
      "\u001b[34mTime to load cpu_adam op: 30.472198963165283 seconds\u001b[0m\n",
      "\u001b[34mLoading extension module cpu_adam...\u001b[0m\n",
      "\u001b[34mTime to load cpu_adam op: 30.38381052017212 seconds\u001b[0m\n",
      "\u001b[34mLoading extension module cpu_adam...\u001b[0m\n",
      "\u001b[34mTime to load cpu_adam op: 30.403457164764404 seconds\u001b[0m\n",
      "\u001b[34mParameter Offload: Total persistent parameters: 266240 in 65 params\u001b[0m\n",
      "\u001b[34m[2023-07-02 09:08:20.587: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.27.0.dev0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[2023-07-02 09:08:20.587: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.27.0.dev0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[2023-07-02 09:08:20.587: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.27.0.dev0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[2023-07-02 09:08:20.588: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.27.0.dev0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[2023-07-02 09:08:20.588: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.27.0.dev0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[2023-07-02 09:08:20.588: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.27.0.dev0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[2023-07-02 09:08:20.589: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.27.0.dev0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[2023-07-02 09:08:20.616 algo-1:645 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2023-07-02 09:08:20.616 algo-1:642 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2023-07-02 09:08:20.616 algo-1:644 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2023-07-02 09:08:20.617 algo-1:643 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2023-07-02 09:08:20.617 algo-1:647 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2023-07-02 09:08:20.618 algo-1:641 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2023-07-02 09:08:20.618 algo-1:646 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2023-07-02 09:08:20.637 algo-1:644 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\u001b[0m\n",
      "\u001b[34m[2023-07-02 09:08:20.637 algo-1:645 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\u001b[0m\n",
      "\u001b[34m[2023-07-02 09:08:20.637 algo-1:642 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\u001b[0m\n",
      "\u001b[34m[2023-07-02 09:08:20.637 algo-1:643 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\u001b[0m\n",
      "\u001b[34m[2023-07-02 09:08:20.638 algo-1:647 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\u001b[0m\n",
      "\u001b[34m[2023-07-02 09:08:20.638 algo-1:641 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\u001b[0m\n",
      "\u001b[34m[2023-07-02 09:08:20.638 algo-1:646 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\u001b[0m\n",
      "\u001b[34m0%|          | 0/609 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[2023-07-02 09:08:20.843: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.27.0.dev0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[2023-07-02 09:08:20.873 algo-1:640 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2023-07-02 09:08:20.894 algo-1:640 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\u001b[0m\n",
      "\u001b[34m0%|          | 1/609 [00:21<3:36:16, 21.34s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 2/609 [00:41<3:31:02, 20.86s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 3/609 [01:02<3:27:42, 20.56s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 4/609 [01:22<3:27:16, 20.56s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 5/609 [01:43<3:26:30, 20.51s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 6/609 [02:03<3:26:00, 20.50s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 7/609 [02:23<3:24:06, 20.34s/it]\u001b[0m\n",
      "\u001b[34m1%|▏         | 8/609 [02:43<3:23:24, 20.31s/it]\u001b[0m\n",
      "\u001b[34m1%|▏         | 9/609 [03:03<3:21:50, 20.18s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 10/609 [03:23<3:20:27, 20.08s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 11/609 [03:43<3:20:00, 20.07s/it]\u001b[0m\n",
      "\u001b[34m[2023-07-02 09:12:25,055] [WARNING] [stage3.py:1850:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m2%|▏         | 12/609 [04:04<3:21:39, 20.27s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 13/609 [04:24<3:20:57, 20.23s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 14/609 [04:44<3:20:32, 20.22s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 15/609 [05:04<3:20:27, 20.25s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 16/609 [05:25<3:20:09, 20.25s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 17/609 [05:45<3:19:58, 20.27s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 18/609 [06:06<3:21:10, 20.42s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 19/609 [06:26<3:20:11, 20.36s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 20/609 [06:46<3:19:16, 20.30s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 21/609 [07:06<3:18:53, 20.30s/it]\u001b[0m\n",
      "\u001b[34m4%|▎         | 22/609 [07:27<3:18:51, 20.33s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 23/609 [07:47<3:19:08, 20.39s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 24/609 [08:08<3:18:33, 20.37s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 25/609 [08:28<3:17:56, 20.34s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 26/609 [08:49<3:18:21, 20.41s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 27/609 [09:09<3:17:34, 20.37s/it]\u001b[0m\n",
      "\u001b[34m5%|▍         | 28/609 [09:30<3:18:08, 20.46s/it]\u001b[0m\n",
      "\u001b[34m5%|▍         | 29/609 [09:50<3:17:48, 20.46s/it]\u001b[0m\n",
      "\u001b[34m5%|▍         | 30/609 [10:10<3:17:00, 20.42s/it]\u001b[0m\n",
      "\u001b[34m5%|▌         | 31/609 [10:31<3:17:06, 20.46s/it]\u001b[0m\n",
      "\u001b[34m5%|▌         | 32/609 [10:51<3:16:38, 20.45s/it]\u001b[0m\n",
      "\u001b[34m5%|▌         | 33/609 [11:12<3:17:00, 20.52s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 34/609 [11:32<3:16:31, 20.51s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 35/609 [11:53<3:15:55, 20.48s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 36/609 [12:13<3:15:21, 20.46s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 37/609 [12:34<3:14:39, 20.42s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 38/609 [12:54<3:14:58, 20.49s/it]\u001b[0m\n",
      "\u001b[34m6%|▋         | 39/609 [13:15<3:14:28, 20.47s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 40/609 [13:35<3:13:51, 20.44s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 41/609 [13:55<3:13:15, 20.41s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 42/609 [14:16<3:12:36, 20.38s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 43/609 [14:36<3:12:08, 20.37s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 44/609 [14:57<3:13:29, 20.55s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 45/609 [15:17<3:12:36, 20.49s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 46/609 [15:38<3:12:03, 20.47s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 47/609 [15:58<3:11:41, 20.47s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 48/609 [16:19<3:11:05, 20.44s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 49/609 [16:39<3:11:35, 20.53s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 50/609 [17:00<3:11:02, 20.50s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 51/609 [17:20<3:10:24, 20.47s/it]\u001b[0m\n",
      "\u001b[34m9%|▊         | 52/609 [17:41<3:09:53, 20.46s/it]\u001b[0m\n",
      "\u001b[34m9%|▊         | 53/609 [18:01<3:10:43, 20.58s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 54/609 [18:22<3:10:21, 20.58s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 55/609 [18:42<3:08:55, 20.46s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 56/609 [19:02<3:07:52, 20.38s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 57/609 [19:23<3:07:29, 20.38s/it]\u001b[0m\n",
      "\u001b[34m10%|▉         | 58/609 [19:43<3:06:43, 20.33s/it]\u001b[0m\n",
      "\u001b[34m10%|▉         | 59/609 [20:03<3:06:02, 20.30s/it]\u001b[0m\n",
      "\u001b[34m10%|▉         | 60/609 [20:24<3:07:14, 20.46s/it]\u001b[0m\n",
      "\u001b[34m10%|█         | 61/609 [20:45<3:06:49, 20.46s/it]\u001b[0m\n",
      "\u001b[34m10%|█         | 62/609 [21:05<3:06:07, 20.42s/it]\u001b[0m\n",
      "\u001b[34m10%|█         | 63/609 [21:25<3:05:57, 20.43s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 64/609 [21:46<3:05:41, 20.44s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 65/609 [22:07<3:06:11, 20.54s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 66/609 [22:27<3:05:46, 20.53s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 67/609 [22:48<3:05:38, 20.55s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 68/609 [23:08<3:05:11, 20.54s/it]\u001b[0m\n",
      "\u001b[34m11%|█▏        | 69/609 [23:29<3:04:46, 20.53s/it]\u001b[0m\n",
      "\u001b[34m11%|█▏        | 70/609 [23:49<3:04:40, 20.56s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 71/609 [24:10<3:04:14, 20.55s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 72/609 [24:30<3:03:37, 20.52s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 73/609 [24:51<3:02:54, 20.48s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 74/609 [25:11<3:02:16, 20.44s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 75/609 [25:31<3:01:37, 20.41s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 76/609 [25:52<3:02:14, 20.52s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 77/609 [26:12<3:01:20, 20.45s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 78/609 [26:33<3:00:44, 20.42s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 79/609 [26:53<2:59:55, 20.37s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 80/609 [27:13<2:59:34, 20.37s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 81/609 [27:34<2:59:48, 20.43s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 82/609 [27:54<2:59:12, 20.40s/it]\u001b[0m\n",
      "\u001b[34m14%|█▎        | 83/609 [28:15<2:58:23, 20.35s/it]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 84/609 [28:35<2:57:58, 20.34s/it]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 85/609 [28:55<2:57:29, 20.32s/it]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 86/609 [29:16<2:57:36, 20.38s/it]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 87/609 [29:36<2:57:02, 20.35s/it]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 88/609 [29:56<2:56:21, 20.31s/it]\u001b[0m\n",
      "\u001b[34m15%|█▍        | 89/609 [30:16<2:55:48, 20.29s/it]\u001b[0m\n",
      "\u001b[34m15%|█▍        | 90/609 [30:37<2:55:25, 20.28s/it]\u001b[0m\n",
      "\u001b[34m15%|█▍        | 91/609 [30:57<2:56:06, 20.40s/it]\u001b[0m\n",
      "\u001b[34m15%|█▌        | 92/609 [31:18<2:55:56, 20.42s/it]\u001b[0m\n",
      "\u001b[34m15%|█▌        | 93/609 [31:38<2:55:15, 20.38s/it]\u001b[0m\n",
      "\u001b[34m15%|█▌        | 94/609 [31:58<2:54:45, 20.36s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 95/609 [32:19<2:54:31, 20.37s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 96/609 [32:39<2:54:08, 20.37s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 97/609 [33:00<2:54:23, 20.44s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 98/609 [33:20<2:53:48, 20.41s/it]\u001b[0m\n",
      "\u001b[34m16%|█▋        | 99/609 [33:40<2:53:18, 20.39s/it]\u001b[0m\n",
      "\u001b[34m16%|█▋        | 100/609 [34:01<2:52:32, 20.34s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 101/609 [34:21<2:52:15, 20.35s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 102/609 [34:42<2:52:29, 20.41s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 103/609 [35:02<2:51:47, 20.37s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 104/609 [35:22<2:51:25, 20.37s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 105/609 [35:42<2:50:43, 20.32s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 106/609 [36:03<2:50:00, 20.28s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 107/609 [36:23<2:50:49, 20.42s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 108/609 [36:44<2:50:17, 20.39s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 109/609 [37:04<2:49:28, 20.34s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 110/609 [37:24<2:49:02, 20.33s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 111/609 [37:44<2:48:35, 20.31s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 112/609 [38:05<2:48:01, 20.29s/it]\u001b[0m\n",
      "\u001b[34m19%|█▊        | 113/609 [38:25<2:48:19, 20.36s/it]\u001b[0m\n",
      "\u001b[34m19%|█▊        | 114/609 [38:46<2:47:53, 20.35s/it]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 115/609 [39:06<2:47:26, 20.34s/it]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 116/609 [39:26<2:47:16, 20.36s/it]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 117/609 [39:47<2:46:50, 20.35s/it]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 118/609 [40:07<2:47:16, 20.44s/it]\u001b[0m\n",
      "\u001b[34m20%|█▉        | 119/609 [40:28<2:46:50, 20.43s/it]\u001b[0m\n",
      "\u001b[34m20%|█▉        | 120/609 [40:48<2:46:16, 20.40s/it]\u001b[0m\n",
      "\u001b[34m20%|█▉        | 121/609 [41:08<2:45:55, 20.40s/it]\u001b[0m\n",
      "\u001b[34m20%|██        | 122/609 [41:29<2:45:38, 20.41s/it]\u001b[0m\n",
      "\u001b[34m20%|██        | 123/609 [41:50<2:46:13, 20.52s/it]\u001b[0m\n",
      "\u001b[34m20%|██        | 124/609 [42:10<2:45:30, 20.48s/it]\u001b[0m\n",
      "\u001b[34m21%|██        | 125/609 [42:31<2:45:25, 20.51s/it]\u001b[0m\n",
      "\u001b[34m21%|██        | 126/609 [42:51<2:44:37, 20.45s/it]\u001b[0m\n",
      "\u001b[34m21%|██        | 127/609 [43:11<2:44:13, 20.44s/it]\u001b[0m\n",
      "\u001b[34m21%|██        | 128/609 [43:32<2:43:37, 20.41s/it]\u001b[0m\n",
      "\u001b[34m21%|██        | 129/609 [43:52<2:43:51, 20.48s/it]\u001b[0m\n",
      "\u001b[34m21%|██▏       | 130/609 [44:13<2:43:16, 20.45s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 131/609 [44:33<2:42:34, 20.41s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 132/609 [44:53<2:42:03, 20.39s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 133/609 [45:14<2:41:36, 20.37s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 134/609 [45:34<2:41:43, 20.43s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 135/609 [45:55<2:41:19, 20.42s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 136/609 [46:15<2:41:04, 20.43s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 137/609 [46:35<2:40:18, 20.38s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 138/609 [46:56<2:39:52, 20.37s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 139/609 [47:16<2:40:34, 20.50s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 140/609 [47:37<2:39:39, 20.43s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 141/609 [47:57<2:39:04, 20.39s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 142/609 [48:17<2:38:40, 20.39s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 143/609 [48:38<2:38:02, 20.35s/it]\u001b[0m\n",
      "\u001b[34m24%|██▎       | 144/609 [48:58<2:37:23, 20.31s/it]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 145/609 [49:18<2:37:33, 20.37s/it]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 146/609 [49:39<2:37:08, 20.36s/it]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 147/609 [49:59<2:36:48, 20.36s/it]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 148/609 [50:19<2:36:31, 20.37s/it]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 149/609 [50:40<2:36:04, 20.36s/it]\u001b[0m\n",
      "\u001b[34m25%|██▍       | 150/609 [51:00<2:36:07, 20.41s/it]\u001b[0m\n",
      "\u001b[34m25%|██▍       | 151/609 [51:21<2:35:23, 20.36s/it]\u001b[0m\n",
      "\u001b[34m25%|██▍       | 152/609 [51:41<2:34:54, 20.34s/it]\u001b[0m\n",
      "\u001b[34m25%|██▌       | 153/609 [52:01<2:34:24, 20.32s/it]\u001b[0m\n",
      "\u001b[34m25%|██▌       | 154/609 [52:21<2:33:53, 20.29s/it]\u001b[0m\n",
      "\u001b[34m25%|██▌       | 155/609 [52:42<2:34:32, 20.42s/it]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 156/609 [53:02<2:33:41, 20.36s/it]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 157/609 [53:23<2:33:04, 20.32s/it]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 158/609 [53:43<2:33:02, 20.36s/it]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 159/609 [54:03<2:32:21, 20.32s/it]\u001b[0m\n",
      "\u001b[34m26%|██▋       | 160/609 [54:24<2:32:18, 20.35s/it]\u001b[0m\n",
      "\u001b[34m26%|██▋       | 161/609 [54:44<2:31:16, 20.26s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 162/609 [55:04<2:31:05, 20.28s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 163/609 [55:24<2:30:57, 20.31s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 164/609 [55:45<2:30:31, 20.30s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 165/609 [56:05<2:30:35, 20.35s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 166/609 [56:25<2:29:50, 20.29s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 167/609 [56:46<2:29:49, 20.34s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 168/609 [57:06<2:29:38, 20.36s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 169/609 [57:27<2:29:22, 20.37s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 170/609 [57:47<2:28:53, 20.35s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 171/609 [58:08<2:29:29, 20.48s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 172/609 [58:28<2:28:45, 20.42s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 173/609 [58:48<2:28:15, 20.40s/it]\u001b[0m\n",
      "\u001b[34m29%|██▊       | 174/609 [59:09<2:28:09, 20.44s/it]\u001b[0m\n",
      "\u001b[34m29%|██▊       | 175/609 [59:29<2:27:14, 20.36s/it]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 176/609 [59:50<2:27:42, 20.47s/it]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 177/609 [1:00:10<2:26:57, 20.41s/it]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 178/609 [1:00:30<2:26:16, 20.36s/it]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 179/609 [1:00:50<2:25:34, 20.31s/it]\u001b[0m\n",
      "\u001b[34m30%|██▉       | 180/609 [1:01:11<2:25:00, 20.28s/it]\u001b[0m\n",
      "\u001b[34m30%|██▉       | 181/609 [1:01:31<2:25:02, 20.33s/it]\u001b[0m\n",
      "\u001b[34m30%|██▉       | 182/609 [1:01:51<2:24:41, 20.33s/it]\u001b[0m\n",
      "\u001b[34m30%|███       | 183/609 [1:02:12<2:24:30, 20.35s/it]\u001b[0m\n",
      "\u001b[34m30%|███       | 184/609 [1:02:32<2:24:17, 20.37s/it]\u001b[0m\n",
      "\u001b[34m30%|███       | 185/609 [1:02:53<2:24:24, 20.43s/it]\u001b[0m\n",
      "\u001b[34m31%|███       | 186/609 [1:03:13<2:24:01, 20.43s/it]\u001b[0m\n",
      "\u001b[34m31%|███       | 187/609 [1:03:34<2:24:22, 20.53s/it]\u001b[0m\n",
      "\u001b[34m31%|███       | 188/609 [1:03:55<2:24:06, 20.54s/it]\u001b[0m\n",
      "\u001b[34m31%|███       | 189/609 [1:04:15<2:23:31, 20.50s/it]\u001b[0m\n",
      "\u001b[34m31%|███       | 190/609 [1:04:35<2:23:00, 20.48s/it]\u001b[0m\n",
      "\u001b[34m31%|███▏      | 191/609 [1:04:56<2:22:28, 20.45s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 192/609 [1:05:16<2:22:32, 20.51s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 193/609 [1:05:37<2:21:49, 20.45s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 194/609 [1:05:57<2:21:20, 20.43s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 195/609 [1:06:18<2:20:55, 20.42s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 196/609 [1:06:38<2:20:28, 20.41s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 197/609 [1:06:59<2:20:37, 20.48s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 198/609 [1:07:19<2:19:53, 20.42s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 199/609 [1:07:39<2:19:17, 20.38s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 200/609 [1:08:00<2:19:03, 20.40s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 201/609 [1:08:20<2:19:02, 20.45s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 202/609 [1:08:40<2:18:25, 20.41s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 203/609 [1:09:01<2:18:47, 20.51s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 204/609 [1:09:24<2:23:43, 21.29s/it]\u001b[0m\n",
      "\u001b[34m34%|███▎      | 205/609 [1:09:45<2:21:43, 21.05s/it]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 206/609 [1:10:05<2:20:01, 20.85s/it]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 207/609 [1:10:26<2:18:45, 20.71s/it]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 208/609 [1:10:46<2:18:26, 20.71s/it]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 209/609 [1:11:07<2:17:29, 20.62s/it]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 210/609 [1:11:27<2:16:33, 20.53s/it]\u001b[0m\n",
      "\u001b[34m35%|███▍      | 211/609 [1:11:47<2:15:37, 20.45s/it]\u001b[0m\n",
      "\u001b[34m35%|███▍      | 212/609 [1:12:07<2:14:51, 20.38s/it]\u001b[0m\n",
      "\u001b[34m35%|███▍      | 213/609 [1:12:28<2:14:46, 20.42s/it]\u001b[0m\n",
      "\u001b[34m35%|███▌      | 214/609 [1:12:48<2:14:07, 20.37s/it]\u001b[0m\n",
      "\u001b[34m35%|███▌      | 215/609 [1:13:09<2:13:37, 20.35s/it]\u001b[0m\n",
      "\u001b[34m35%|███▌      | 216/609 [1:13:29<2:12:57, 20.30s/it]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 217/609 [1:13:49<2:12:22, 20.26s/it]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 218/609 [1:14:10<2:13:00, 20.41s/it]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 219/609 [1:14:30<2:12:37, 20.40s/it]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 220/609 [1:14:50<2:11:54, 20.35s/it]\u001b[0m\n",
      "\u001b[34m36%|███▋      | 221/609 [1:15:11<2:11:33, 20.34s/it]\u001b[0m\n",
      "\u001b[34m36%|███▋      | 222/609 [1:15:31<2:11:13, 20.35s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 223/609 [1:15:51<2:10:55, 20.35s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 224/609 [1:16:12<2:11:03, 20.43s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 225/609 [1:16:32<2:10:25, 20.38s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 226/609 [1:16:52<2:09:49, 20.34s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 227/609 [1:17:13<2:09:24, 20.32s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 228/609 [1:17:33<2:09:08, 20.34s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 229/609 [1:17:54<2:09:20, 20.42s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 230/609 [1:18:14<2:08:49, 20.40s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 231/609 [1:18:34<2:08:29, 20.40s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 232/609 [1:18:55<2:07:59, 20.37s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 233/609 [1:19:15<2:07:35, 20.36s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 234/609 [1:19:36<2:08:15, 20.52s/it]\u001b[0m\n",
      "\u001b[34m39%|███▊      | 235/609 [1:19:56<2:07:32, 20.46s/it]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 236/609 [1:20:17<2:07:09, 20.45s/it]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 237/609 [1:20:37<2:06:39, 20.43s/it]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 238/609 [1:20:57<2:06:13, 20.41s/it]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 239/609 [1:21:18<2:06:00, 20.43s/it]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 240/609 [1:21:39<2:05:58, 20.48s/it]\u001b[0m\n",
      "\u001b[34m40%|███▉      | 241/609 [1:21:59<2:05:21, 20.44s/it]\u001b[0m\n",
      "\u001b[34m40%|███▉      | 242/609 [1:22:19<2:04:55, 20.42s/it]\u001b[0m\n",
      "\u001b[34m40%|███▉      | 243/609 [1:22:40<2:04:24, 20.39s/it]\u001b[0m\n",
      "\u001b[34m40%|████      | 244/609 [1:23:00<2:04:03, 20.39s/it]\u001b[0m\n",
      "\u001b[34m40%|████      | 245/609 [1:23:21<2:03:54, 20.42s/it]\u001b[0m\n",
      "\u001b[34m40%|████      | 246/609 [1:23:41<2:03:21, 20.39s/it]\u001b[0m\n",
      "\u001b[34m41%|████      | 247/609 [1:24:01<2:02:50, 20.36s/it]\u001b[0m\n",
      "\u001b[34m41%|████      | 248/609 [1:24:21<2:02:14, 20.32s/it]\u001b[0m\n",
      "\u001b[34m41%|████      | 249/609 [1:24:42<2:01:49, 20.30s/it]\u001b[0m\n",
      "\u001b[34m41%|████      | 250/609 [1:25:03<2:02:38, 20.50s/it]\u001b[0m\n",
      "\u001b[34m41%|████      | 251/609 [1:25:23<2:01:57, 20.44s/it]\u001b[0m\n",
      "\u001b[34m41%|████▏     | 252/609 [1:25:43<2:01:26, 20.41s/it]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 253/609 [1:26:03<2:00:48, 20.36s/it]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 254/609 [1:26:24<2:00:27, 20.36s/it]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 255/609 [1:26:44<2:00:30, 20.43s/it]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 256/609 [1:27:05<1:59:42, 20.35s/it]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 257/609 [1:27:25<1:59:12, 20.32s/it]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 258/609 [1:27:45<1:58:58, 20.34s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 259/609 [1:28:06<1:58:40, 20.34s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 260/609 [1:28:26<1:58:43, 20.41s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 261/609 [1:28:46<1:58:01, 20.35s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 262/609 [1:29:07<1:57:37, 20.34s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 263/609 [1:29:27<1:57:27, 20.37s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 264/609 [1:29:47<1:57:12, 20.38s/it]\u001b[0m\n",
      "\u001b[34m44%|████▎     | 265/609 [1:30:08<1:56:53, 20.39s/it]\u001b[0m\n",
      "\u001b[34m44%|████▎     | 266/609 [1:30:29<1:57:06, 20.49s/it]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 267/609 [1:30:49<1:56:32, 20.45s/it]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 268/609 [1:31:09<1:55:59, 20.41s/it]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 269/609 [1:31:30<1:55:22, 20.36s/it]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 270/609 [1:31:50<1:54:46, 20.31s/it]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 271/609 [1:32:10<1:54:31, 20.33s/it]\u001b[0m\n",
      "\u001b[34m45%|████▍     | 272/609 [1:32:30<1:53:57, 20.29s/it]\u001b[0m\n",
      "\u001b[34m45%|████▍     | 273/609 [1:32:51<1:53:32, 20.27s/it]\u001b[0m\n",
      "\u001b[34m45%|████▍     | 274/609 [1:33:11<1:52:58, 20.23s/it]\u001b[0m\n",
      "\u001b[34m45%|████▌     | 275/609 [1:33:31<1:52:43, 20.25s/it]\u001b[0m\n",
      "\u001b[34m45%|████▌     | 276/609 [1:33:51<1:52:40, 20.30s/it]\u001b[0m\n",
      "\u001b[34m45%|████▌     | 277/609 [1:34:11<1:52:00, 20.24s/it]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 278/609 [1:34:32<1:51:41, 20.25s/it]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 279/609 [1:34:52<1:51:23, 20.25s/it]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 280/609 [1:35:12<1:51:17, 20.30s/it]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 281/609 [1:35:33<1:51:07, 20.33s/it]\u001b[0m\n",
      "\u001b[34m46%|████▋     | 282/609 [1:35:54<1:51:42, 20.50s/it]\u001b[0m\n",
      "\u001b[34m46%|████▋     | 283/609 [1:36:14<1:51:00, 20.43s/it]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 284/609 [1:36:34<1:50:32, 20.41s/it]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 285/609 [1:36:55<1:50:11, 20.40s/it]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 286/609 [1:37:15<1:49:27, 20.33s/it]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 287/609 [1:37:35<1:49:34, 20.42s/it]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 288/609 [1:37:56<1:48:49, 20.34s/it]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 289/609 [1:38:16<1:48:12, 20.29s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 290/609 [1:38:36<1:48:04, 20.33s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 291/609 [1:38:56<1:47:35, 20.30s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 292/609 [1:39:17<1:47:47, 20.40s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 293/609 [1:39:37<1:47:24, 20.39s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 294/609 [1:39:58<1:46:58, 20.37s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 295/609 [1:40:18<1:46:30, 20.35s/it]\u001b[0m\n",
      "\u001b[34m49%|████▊     | 296/609 [1:40:38<1:46:03, 20.33s/it]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 297/609 [1:40:59<1:45:43, 20.33s/it]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 298/609 [1:41:20<1:46:06, 20.47s/it]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 299/609 [1:41:40<1:45:33, 20.43s/it]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 300/609 [1:42:00<1:45:17, 20.44s/it]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 301/609 [1:42:21<1:44:45, 20.41s/it]\u001b[0m\n",
      "\u001b[34m50%|████▉     | 302/609 [1:42:41<1:44:21, 20.40s/it]\u001b[0m\n",
      "\u001b[34m50%|████▉     | 303/609 [1:43:02<1:44:16, 20.45s/it]\u001b[0m\n",
      "\u001b[34m50%|████▉     | 304/609 [1:43:22<1:43:31, 20.36s/it]\u001b[0m\n",
      "\u001b[34m50%|█████     | 305/609 [1:43:42<1:43:19, 20.39s/it]\u001b[0m\n",
      "\u001b[34m50%|█████     | 306/609 [1:44:03<1:43:14, 20.44s/it]\u001b[0m\n",
      "\u001b[34m50%|█████     | 307/609 [1:44:23<1:43:07, 20.49s/it]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from sagemaker.estimator import Estimator\n",
    "\n",
    "environment = {\n",
    "              'MODEL_S3_BUCKET': sagemaker_default_bucket # The bucket to store pretrained model and fine-tune model\n",
    "}\n",
    "\n",
    "base_job_name = 'stanford-alpaca-demo'\n",
    "\n",
    "instance_type = 'ml.p4d.24xlarge'\n",
    "\n",
    "estimator = Estimator(role=role,\n",
    "                      entry_point='train.sh',\n",
    "                      source_dir='./src',\n",
    "                      base_job_name=base_job_name,\n",
    "                      instance_count=1,\n",
    "                      instance_type=instance_type,\n",
    "                      image_uri=image_uri,\n",
    "                      environment=environment,\n",
    "                      keep_alive_period_in_seconds=3600,\n",
    "                      disable_profiler=True,\n",
    "                      debugger_hook_config=False)\n",
    "\n",
    "estimator.fit()\n",
    "\n",
    "# if use input channel, SageMaker automatically the data to training instances, path is e.g. /opt/ml/input/channel_train/\n",
    "# inputs = {\"channel_train\":\"s3://xxx/xxx\",\"channel_eval\":\"s3://xxx/yyy\"}\n",
    "# estimator.fit(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd830bcc",
   "metadata": {},
   "source": [
    "## Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9037587",
   "metadata": {},
   "source": [
    "[SageMaker Python SDK](https://sagemaker.readthedocs.io/en/stable/api/training/estimators.html#sagemaker.estimator.EstimatorBase)\n",
    "\n",
    "[DeepSpeed Configuration JSON](https://www.deepspeed.ai/docs/config-json/)\n",
    "\n",
    "[SageMaker Examples](https://github.com/aws/amazon-sagemaker-examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b764b0fe-785e-419d-8fb0-faff0dbd778f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.m5.xlarge",
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
